{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.3 64-bit (conda)",
   "metadata": {
    "interpreter": {
     "hash": "23253deaa44fc8808375d0438997a72d943f96b17c9c832de56565a7ac5f0797"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "     C% Biomass  H% Biomass  O% Biomass  N% Biomass  C% HDPE  H% HDPE  \\\n",
       "0         63.85         8.6        3.74        0.33    85.72    14.28   \n",
       "1         63.85         8.6        3.74        0.33    85.72    14.28   \n",
       "2         63.85         8.6        3.74        0.33    85.72    14.28   \n",
       "3         63.85         8.6        3.74        0.33    85.72    14.28   \n",
       "4         63.85         8.6        3.74        0.33    85.72    14.28   \n",
       "..          ...         ...         ...         ...      ...      ...   \n",
       "242       58.00         6.3       34.00        2.40     0.00     0.00   \n",
       "243       58.00         6.3       34.00        2.40     0.00     0.00   \n",
       "244       58.00         6.3       34.00        2.40     0.00     0.00   \n",
       "245       58.00         6.3       34.00        2.40     0.00     0.00   \n",
       "246       58.00         6.3       34.00        2.40     0.00     0.00   \n",
       "\n",
       "     O% HDPE  N% HDPE  C% PS  H% PS  O% PS  N% PS  Time  Rate  Ratio  Temp  \\\n",
       "0          0      0.0    0.0    0.0    0.0      0   0.5     5   66.7   500   \n",
       "1          0      0.0    0.0    0.0    0.0      0   0.5     5  100.0   500   \n",
       "2          0      0.0    0.0    0.0    0.0      0   0.5     5   50.0   500   \n",
       "3          0      0.0    0.0    0.0    0.0      0   0.5     5   33.3   500   \n",
       "4          0      0.0    0.0    0.0    0.0      0   0.5     5    0.0   500   \n",
       "..       ...      ...    ...    ...    ...    ...   ...   ...    ...   ...   \n",
       "242        0      0.0   91.0    8.8    0.3      0  30.0   100   60.0   550   \n",
       "243        0      0.0   91.0    8.8    0.3      0  30.0   100   61.5   550   \n",
       "244        0      0.0   91.0    8.8    0.3      0  30.0   100   63.0   550   \n",
       "245        0      0.0   91.0    8.8    0.3      0  30.0   100   64.5   550   \n",
       "246        0      0.0   91.0    8.8    0.3      0  30.0   100   66.0   550   \n",
       "\n",
       "          Oil%      Char%       Gas%  \n",
       "0    60.156065  10.936392  28.907543  \n",
       "1    56.000000   0.000000  44.000000  \n",
       "2    45.993999  17.313035  36.692966  \n",
       "3    44.038082  23.267154  32.694764  \n",
       "4    25.762381  34.435228  39.802391  \n",
       "..         ...        ...        ...  \n",
       "242  16.633680  15.340660  13.907280  \n",
       "243  15.999231  14.779844  13.749581  \n",
       "244  15.349117  14.221099  13.605475  \n",
       "245  14.683338  13.664426  13.474963  \n",
       "246  14.001893  13.109825  13.358045  \n",
       "\n",
       "[247 rows x 19 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>C% Biomass</th>\n      <th>H% Biomass</th>\n      <th>O% Biomass</th>\n      <th>N% Biomass</th>\n      <th>C% HDPE</th>\n      <th>H% HDPE</th>\n      <th>O% HDPE</th>\n      <th>N% HDPE</th>\n      <th>C% PS</th>\n      <th>H% PS</th>\n      <th>O% PS</th>\n      <th>N% PS</th>\n      <th>Time</th>\n      <th>Rate</th>\n      <th>Ratio</th>\n      <th>Temp</th>\n      <th>Oil%</th>\n      <th>Char%</th>\n      <th>Gas%</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>63.85</td>\n      <td>8.6</td>\n      <td>3.74</td>\n      <td>0.33</td>\n      <td>85.72</td>\n      <td>14.28</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0.5</td>\n      <td>5</td>\n      <td>66.7</td>\n      <td>500</td>\n      <td>60.156065</td>\n      <td>10.936392</td>\n      <td>28.907543</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>63.85</td>\n      <td>8.6</td>\n      <td>3.74</td>\n      <td>0.33</td>\n      <td>85.72</td>\n      <td>14.28</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0.5</td>\n      <td>5</td>\n      <td>100.0</td>\n      <td>500</td>\n      <td>56.000000</td>\n      <td>0.000000</td>\n      <td>44.000000</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>63.85</td>\n      <td>8.6</td>\n      <td>3.74</td>\n      <td>0.33</td>\n      <td>85.72</td>\n      <td>14.28</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0.5</td>\n      <td>5</td>\n      <td>50.0</td>\n      <td>500</td>\n      <td>45.993999</td>\n      <td>17.313035</td>\n      <td>36.692966</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>63.85</td>\n      <td>8.6</td>\n      <td>3.74</td>\n      <td>0.33</td>\n      <td>85.72</td>\n      <td>14.28</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0.5</td>\n      <td>5</td>\n      <td>33.3</td>\n      <td>500</td>\n      <td>44.038082</td>\n      <td>23.267154</td>\n      <td>32.694764</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>63.85</td>\n      <td>8.6</td>\n      <td>3.74</td>\n      <td>0.33</td>\n      <td>85.72</td>\n      <td>14.28</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0.5</td>\n      <td>5</td>\n      <td>0.0</td>\n      <td>500</td>\n      <td>25.762381</td>\n      <td>34.435228</td>\n      <td>39.802391</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>242</th>\n      <td>58.00</td>\n      <td>6.3</td>\n      <td>34.00</td>\n      <td>2.40</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>91.0</td>\n      <td>8.8</td>\n      <td>0.3</td>\n      <td>0</td>\n      <td>30.0</td>\n      <td>100</td>\n      <td>60.0</td>\n      <td>550</td>\n      <td>16.633680</td>\n      <td>15.340660</td>\n      <td>13.907280</td>\n    </tr>\n    <tr>\n      <th>243</th>\n      <td>58.00</td>\n      <td>6.3</td>\n      <td>34.00</td>\n      <td>2.40</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>91.0</td>\n      <td>8.8</td>\n      <td>0.3</td>\n      <td>0</td>\n      <td>30.0</td>\n      <td>100</td>\n      <td>61.5</td>\n      <td>550</td>\n      <td>15.999231</td>\n      <td>14.779844</td>\n      <td>13.749581</td>\n    </tr>\n    <tr>\n      <th>244</th>\n      <td>58.00</td>\n      <td>6.3</td>\n      <td>34.00</td>\n      <td>2.40</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>91.0</td>\n      <td>8.8</td>\n      <td>0.3</td>\n      <td>0</td>\n      <td>30.0</td>\n      <td>100</td>\n      <td>63.0</td>\n      <td>550</td>\n      <td>15.349117</td>\n      <td>14.221099</td>\n      <td>13.605475</td>\n    </tr>\n    <tr>\n      <th>245</th>\n      <td>58.00</td>\n      <td>6.3</td>\n      <td>34.00</td>\n      <td>2.40</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>91.0</td>\n      <td>8.8</td>\n      <td>0.3</td>\n      <td>0</td>\n      <td>30.0</td>\n      <td>100</td>\n      <td>64.5</td>\n      <td>550</td>\n      <td>14.683338</td>\n      <td>13.664426</td>\n      <td>13.474963</td>\n    </tr>\n    <tr>\n      <th>246</th>\n      <td>58.00</td>\n      <td>6.3</td>\n      <td>34.00</td>\n      <td>2.40</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>91.0</td>\n      <td>8.8</td>\n      <td>0.3</td>\n      <td>0</td>\n      <td>30.0</td>\n      <td>100</td>\n      <td>66.0</td>\n      <td>550</td>\n      <td>14.001893</td>\n      <td>13.109825</td>\n      <td>13.358045</td>\n    </tr>\n  </tbody>\n</table>\n<p>247 rows Ã— 19 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "\n",
    "dir_p = r'C:\\Users\\Honeyz\\Desktop\\Aessa\\THE_SIS\\PyroDataProcessed.csv'\n",
    "raw_dataset = pd.read_csv(dir_p, skipinitialspace=True)\n",
    "\n",
    "dataset = raw_dataset.copy()\n",
    "dataset = dataset.dropna()\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "KFold(n_splits=247, random_state=None, shuffle=False) \n",
      "\n",
      "loss for fold 1: \n",
      " Regressor 1: 7.106230988546166\n",
      " Regressor 2:1.8747229935918046\n",
      "Regressor 3:3.352033137069082\n",
      "\n",
      "\n",
      "loss for fold 2: \n",
      " Regressor 1: 8.97034820129366\n",
      " Regressor 2:4.1950424593377065\n",
      "Regressor 3:17.797441567796703\n",
      "\n",
      "\n",
      "loss for fold 3: \n",
      " Regressor 1: 1.2096032125334304\n",
      " Regressor 2:0.057217500171219626\n",
      "Regressor 3:1.4964599555102396\n",
      "\n",
      "\n",
      "loss for fold 4: \n",
      " Regressor 1: 2.685097824425661\n",
      " Regressor 2:1.568224240569144\n",
      "Regressor 3:5.467352929481258\n",
      "\n",
      "\n",
      "loss for fold 5: \n",
      " Regressor 1: 3.9583943135348143\n",
      " Regressor 2:4.149331511851489\n",
      "Regressor 3:4.2917530226827765\n",
      "\n",
      "\n",
      "loss for fold 6: \n",
      " Regressor 1: 3.7661330416468566\n",
      " Regressor 2:4.045000709827278\n",
      "Regressor 3:4.139886827352704\n",
      "\n",
      "\n",
      "loss for fold 7: \n",
      " Regressor 1: 2.96738458207572\n",
      " Regressor 2:3.8439734784505184\n",
      "Regressor 3:3.7811483993439126\n",
      "\n",
      "\n",
      "loss for fold 8: \n",
      " Regressor 1: 2.2459236489868353\n",
      " Regressor 2:3.6002876285622882\n",
      "Regressor 3:3.487234251291042\n",
      "\n",
      "\n",
      "loss for fold 9: \n",
      " Regressor 1: 1.5684501120094225\n",
      " Regressor 2:3.400278601125077\n",
      "Regressor 3:3.254297437405114\n",
      "\n",
      "\n",
      "loss for fold 10: \n",
      " Regressor 1: 0.9336296875086703\n",
      " Regressor 2:3.200119142034989\n",
      "Regressor 3:3.0741963223794\n",
      "\n",
      "\n",
      "loss for fold 11: \n",
      " Regressor 1: 0.3451249914588246\n",
      " Regressor 2:2.8102653351089195\n",
      "Regressor 3:2.899694537770827\n",
      "\n",
      "\n",
      "loss for fold 12: \n",
      " Regressor 1: 0.20249750977276193\n",
      " Regressor 2:2.6177115077460655\n",
      "Regressor 3:2.8134884040183152\n",
      "\n",
      "\n",
      "loss for fold 13: \n",
      " Regressor 1: 0.7036296778134385\n",
      " Regressor 2:2.42311566696932\n",
      "Regressor 3:2.7641530892993202\n",
      "\n",
      "\n",
      "loss for fold 14: \n",
      " Regressor 1: 1.162810041114568\n",
      " Regressor 2:2.2289771215728216\n",
      "Regressor 3:2.7454529071926856\n",
      "\n",
      "\n",
      "loss for fold 15: \n",
      " Regressor 1: 1.577945578705652\n",
      " Regressor 2:2.0368919656403754\n",
      "Regressor 3:2.7510050399841575\n",
      "\n",
      "\n",
      "loss for fold 16: \n",
      " Regressor 1: 1.9483670021954467\n",
      " Regressor 2:1.8421950883771316\n",
      "Regressor 3:2.775833908054132\n",
      "\n",
      "\n",
      "loss for fold 17: \n",
      " Regressor 1: 2.275774669881251\n",
      " Regressor 2:1.6486112613338975\n",
      "Regressor 3:2.8149995607736784\n",
      "\n",
      "\n",
      "loss for fold 18: \n",
      " Regressor 1: 2.560476000446698\n",
      " Regressor 2:1.4545279049204716\n",
      "Regressor 3:2.860676463010769\n",
      "\n",
      "\n",
      "loss for fold 19: \n",
      " Regressor 1: 2.7995341957549726\n",
      " Regressor 2:1.260990163750698\n",
      "Regressor 3:2.9086099062346236\n",
      "\n",
      "\n",
      "loss for fold 20: \n",
      " Regressor 1: 2.9950566132167395\n",
      " Regressor 2:1.0677315944207244\n",
      "Regressor 3:2.952075809146123\n",
      "\n",
      "\n",
      "loss for fold 21: \n",
      " Regressor 1: 3.152282557625149\n",
      " Regressor 2:0.8731194786474745\n",
      "Regressor 3:2.9856137214421494\n",
      "\n",
      "\n",
      "loss for fold 22: \n",
      " Regressor 1: 3.260603771525588\n",
      " Regressor 2:0.6795735730450936\n",
      "Regressor 3:3.0184935819176104\n",
      "\n",
      "\n",
      "loss for fold 23: \n",
      " Regressor 1: 3.3252831500147977\n",
      " Regressor 2:0.48585032814630225\n",
      "Regressor 3:3.014434553009835\n",
      "\n",
      "\n",
      "loss for fold 24: \n",
      " Regressor 1: 3.3460714242883682\n",
      " Regressor 2:0.29245217412490376\n",
      "Regressor 3:2.969272808594873\n",
      "\n",
      "\n",
      "loss for fold 25: \n",
      " Regressor 1: 3.3236179437207483\n",
      " Regressor 2:0.09996270945558905\n",
      "Regressor 3:2.9053163091378167\n",
      "\n",
      "\n",
      "loss for fold 26: \n",
      " Regressor 1: 3.2572324651111018\n",
      " Regressor 2:0.09380054470755184\n",
      "Regressor 3:2.802416340640704\n",
      "\n",
      "\n",
      "loss for fold 27: \n",
      " Regressor 1: 3.146753985461956\n",
      " Regressor 2:0.2888039117994161\n",
      "Regressor 3:2.6545636674095476\n",
      "\n",
      "\n",
      "loss for fold 28: \n",
      " Regressor 1: 2.987638363805857\n",
      " Regressor 2:0.4820733934967336\n",
      "Regressor 3:2.4563882512606092\n",
      "\n",
      "\n",
      "loss for fold 29: \n",
      " Regressor 1: 2.7901543908745055\n",
      " Regressor 2:0.6755916927290002\n",
      "Regressor 3:2.2016928098191393\n",
      "\n",
      "\n",
      "loss for fold 30: \n",
      " Regressor 1: 2.547924848514505\n",
      " Regressor 2:0.8695187835890632\n",
      "Regressor 3:1.8850375780848303\n",
      "\n",
      "\n",
      "loss for fold 31: \n",
      " Regressor 1: 2.262671931355655\n",
      " Regressor 2:1.0630746610609307\n",
      "Regressor 3:1.5007844843954672\n",
      "\n",
      "\n",
      "loss for fold 32: \n",
      " Regressor 1: 1.9335454277660062\n",
      " Regressor 2:1.2568189764606714\n",
      "Regressor 3:1.0418920005332026\n",
      "\n",
      "\n",
      "loss for fold 33: \n",
      " Regressor 1: 1.5600273188472542\n",
      " Regressor 2:1.4501606908019191\n",
      "Regressor 3:0.5036733852496269\n",
      "\n",
      "\n",
      "loss for fold 34: \n",
      " Regressor 1: 1.1435007015957268\n",
      " Regressor 2:1.6437695124924687\n",
      "Regressor 3:0.1199836037076274\n",
      "\n",
      "\n",
      "loss for fold 35: \n",
      " Regressor 1: 0.6826306668537043\n",
      " Regressor 2:1.837744175575379\n",
      "Regressor 3:0.8345185557177004\n",
      "\n",
      "\n",
      "loss for fold 36: \n",
      " Regressor 1: 0.17830512160950462\n",
      " Regressor 2:2.031584218669849\n",
      "Regressor 3:1.6461318172232424\n",
      "\n",
      "\n",
      "loss for fold 37: \n",
      " Regressor 1: 0.3695924429313635\n",
      " Regressor 2:2.2251261406361857\n",
      "Regressor 3:2.5606435383765813\n",
      "\n",
      "\n",
      "loss for fold 38: \n",
      " Regressor 1: 0.9614162995075475\n",
      " Regressor 2:2.4190405161716537\n",
      "Regressor 3:3.6341397141076186\n",
      "\n",
      "\n",
      "loss for fold 39: \n",
      " Regressor 1: 1.597166916783678\n",
      " Regressor 2:2.6131934577472338\n",
      "Regressor 3:4.774457259764841\n",
      "\n",
      "\n",
      "loss for fold 40: \n",
      " Regressor 1: 2.27702405354151\n",
      " Regressor 2:2.8064924317184268\n",
      "Regressor 3:6.034223218241543\n",
      "\n",
      "\n",
      "loss for fold 41: \n",
      " Regressor 1: 3.2185027865260807\n",
      " Regressor 2:3.200131299008161\n",
      "Regressor 3:7.420746811443191\n",
      "\n",
      "\n",
      "loss for fold 42: \n",
      " Regressor 1: 3.991348221510343\n",
      " Regressor 2:3.399961017853037\n",
      "Regressor 3:8.940694474872558\n",
      "\n",
      "\n",
      "loss for fold 43: \n",
      " Regressor 1: 4.807263647157363\n",
      " Regressor 2:3.600569302614465\n",
      "Regressor 3:10.596372372304916\n",
      "\n",
      "\n",
      "loss for fold 44: \n",
      " Regressor 1: 5.667540216818075\n",
      " Regressor 2:3.799784612567586\n",
      "Regressor 3:12.39519553414953\n",
      "\n",
      "\n",
      "loss for fold 45: \n",
      " Regressor 1: 6.571706553139414\n",
      " Regressor 2:4.006851471798415\n",
      "Regressor 3:14.344748006746297\n",
      "\n",
      "\n",
      "loss for fold 46: \n",
      " Regressor 1: 7.51999792018141\n",
      " Regressor 2:4.20754713255228\n",
      "Regressor 3:16.495361700740254\n",
      "\n",
      "\n",
      "loss for fold 47: \n",
      " Regressor 1: 1.0433303954143938\n",
      " Regressor 2:2.9988582139048816\n",
      "Regressor 3:0.5532890129116659\n",
      "\n",
      "\n",
      "loss for fold 48: \n",
      " Regressor 1: 2.4012952191074532\n",
      " Regressor 2:1.0473010653498065\n",
      "Regressor 3:1.0656918122656691\n",
      "\n",
      "\n",
      "loss for fold 49: \n",
      " Regressor 1: 5.349781771524192\n",
      " Regressor 2:0.5051915034176098\n",
      "Regressor 3:4.823316868734157\n",
      "\n",
      "\n",
      "loss for fold 50: \n",
      " Regressor 1: 7.422437226215962\n",
      " Regressor 2:0.8567677156631195\n",
      "Regressor 3:5.868278838845913\n",
      "\n",
      "\n",
      "loss for fold 51: \n",
      " Regressor 1: 13.415160960365554\n",
      " Regressor 2:2.690687132483332\n",
      "Regressor 3:5.626584966969397\n",
      "\n",
      "\n",
      "loss for fold 52: \n",
      " Regressor 1: 1.39871790180338\n",
      " Regressor 2:3.001064343394617\n",
      "Regressor 3:0.3573310167599537\n",
      "\n",
      "\n",
      "loss for fold 53: \n",
      " Regressor 1: 1.7668641074496279\n",
      " Regressor 2:2.6920147889287165\n",
      "Regressor 3:0.3486388271746854\n",
      "\n",
      "\n",
      "loss for fold 54: \n",
      " Regressor 1: 2.1026547126917237\n",
      " Regressor 2:2.396536832638219\n",
      "Regressor 3:0.9739693617464518\n",
      "\n",
      "\n",
      "loss for fold 55: \n",
      " Regressor 1: 2.40572819579765\n",
      " Regressor 2:2.114954200002007\n",
      "Regressor 3:1.521650634065189\n",
      "\n",
      "\n",
      "loss for fold 56: \n",
      " Regressor 1: 2.6759411781617857\n",
      " Regressor 2:1.8476708594968905\n",
      "Regressor 3:1.994815659896446\n",
      "\n",
      "\n",
      "loss for fold 57: \n",
      " Regressor 1: 2.913746898254466\n",
      " Regressor 2:1.5926374950596518\n",
      "Regressor 3:2.3966213280941453\n",
      "\n",
      "\n",
      "loss for fold 58: \n",
      " Regressor 1: 3.2041786271308936\n",
      " Regressor 2:1.351828634459629\n",
      "Regressor 3:2.7305167356948417\n",
      "\n",
      "\n",
      "loss for fold 59: \n",
      " Regressor 1: 3.3752388841274055\n",
      " Regressor 2:1.12476943964365\n",
      "Regressor 3:3.0557341114386887\n",
      "\n",
      "\n",
      "loss for fold 60: \n",
      " Regressor 1: 3.5135787589938374\n",
      " Regressor 2:0.9112020385454649\n",
      "Regressor 3:3.2631738295223727\n",
      "\n",
      "\n",
      "loss for fold 61: \n",
      " Regressor 1: 3.6197547877073504\n",
      " Regressor 2:0.7112376233692608\n",
      "Regressor 3:3.4098454528218305\n",
      "\n",
      "\n",
      "loss for fold 62: \n",
      " Regressor 1: 3.6917212719188726\n",
      " Regressor 2:0.5244780173530401\n",
      "Regressor 3:3.5021370701676915\n",
      "\n",
      "\n",
      "loss for fold 63: \n",
      " Regressor 1: 3.731053889483995\n",
      " Regressor 2:0.35154502299045376\n",
      "Regressor 3:3.5431180572237437\n",
      "\n",
      "\n",
      "loss for fold 64: \n",
      " Regressor 1: 3.7392143183331044\n",
      " Regressor 2:0.1914980098216148\n",
      "Regressor 3:3.534785590855762\n",
      "\n",
      "\n",
      "loss for fold 65: \n",
      " Regressor 1: 3.713329612170881\n",
      " Regressor 2:0.046065734054618446\n",
      "Regressor 3:3.435252645428964\n",
      "\n",
      "\n",
      "loss for fold 66: \n",
      " Regressor 1: 3.654523092361245\n",
      " Regressor 2:0.08626523389154883\n",
      "Regressor 3:3.340468990307592\n",
      "\n",
      "\n",
      "loss for fold 67: \n",
      " Regressor 1: 3.5629278923549776\n",
      " Regressor 2:0.20531010086201462\n",
      "Regressor 3:3.2144095881939414\n",
      "\n",
      "\n",
      "loss for fold 68: \n",
      " Regressor 1: 3.4381395108095774\n",
      " Regressor 2:0.31133449456177686\n",
      "Regressor 3:3.0483042209547087\n",
      "\n",
      "\n",
      "loss for fold 69: \n",
      " Regressor 1: 3.2801221146902577\n",
      " Regressor 2:0.40194139636951576\n",
      "Regressor 3:2.8350249466642623\n",
      "\n",
      "\n",
      "loss for fold 70: \n",
      " Regressor 1: 3.0898452528748024\n",
      " Regressor 2:0.48039340407206055\n",
      "Regressor 3:2.602502578324394\n",
      "\n",
      "\n",
      "loss for fold 71: \n",
      " Regressor 1: 2.775877307632264\n",
      " Regressor 2:0.5447821682312863\n",
      "Regressor 3:2.3437201103086736\n",
      "\n",
      "\n",
      "loss for fold 72: \n",
      " Regressor 1: 2.5196117631838035\n",
      " Regressor 2:0.5962159468800827\n",
      "Regressor 3:2.0622853471615556\n",
      "\n",
      "\n",
      "loss for fold 73: \n",
      " Regressor 1: 2.229861933812856\n",
      " Regressor 2:0.6334099560829465\n",
      "Regressor 3:1.7604036090110924\n",
      "\n",
      "\n",
      "loss for fold 74: \n",
      " Regressor 1: 1.907505000699814\n",
      " Regressor 2:0.6578065104894719\n",
      "Regressor 3:1.4411649844147334\n",
      "\n",
      "\n",
      "loss for fold 75: \n",
      " Regressor 1: 1.5526218132623342\n",
      " Regressor 2:0.6685300878527869\n",
      "Regressor 3:1.1093661981974776\n",
      "\n",
      "\n",
      "loss for fold 76: \n",
      " Regressor 1: 1.1644657861910588\n",
      " Regressor 2:0.6647597506371028\n",
      "Regressor 3:0.7661322272138911\n",
      "\n",
      "\n",
      "loss for fold 77: \n",
      " Regressor 1: 0.742917539474675\n",
      " Regressor 2:0.6481669904467324\n",
      "Regressor 3:0.4158860915669429\n",
      "\n",
      "\n",
      "loss for fold 78: \n",
      " Regressor 1: 0.28891278829931366\n",
      " Regressor 2:0.6178610281826309\n",
      "Regressor 3:0.06237471766823788\n",
      "\n",
      "\n",
      "loss for fold 79: \n",
      " Regressor 1: 0.1984167205743148\n",
      " Regressor 2:0.5740134132254262\n",
      "Regressor 3:0.292511412970045\n",
      "\n",
      "\n",
      "loss for fold 80: \n",
      " Regressor 1: 0.7185046416628467\n",
      " Regressor 2:0.5162022658167569\n",
      "Regressor 3:0.6456898346504474\n",
      "\n",
      "\n",
      "loss for fold 81: \n",
      " Regressor 1: 1.2710384767921283\n",
      " Regressor 2:0.44597907025429073\n",
      "Regressor 3:0.9925368429080272\n",
      "\n",
      "\n",
      "loss for fold 82: \n",
      " Regressor 1: 1.8563255005608852\n",
      " Regressor 2:0.3626520425748119\n",
      "Regressor 3:1.3313713577787034\n",
      "\n",
      "\n",
      "loss for fold 83: \n",
      " Regressor 1: 2.475406001089965\n",
      " Regressor 2:0.2631845629289682\n",
      "Regressor 3:1.6580257352780539\n",
      "\n",
      "\n",
      "loss for fold 84: \n",
      " Regressor 1: 3.266680189628829\n",
      " Regressor 2:0.150686654274244\n",
      "Regressor 3:1.9686852144671754\n",
      "\n",
      "\n",
      "loss for fold 85: \n",
      " Regressor 1: 3.9522093141554535\n",
      " Regressor 2:0.025611927004018042\n",
      "Regressor 3:2.262489838768376\n",
      "\n",
      "\n",
      "loss for fold 86: \n",
      " Regressor 1: 4.670440044865828\n",
      " Regressor 2:0.11207156606637092\n",
      "Regressor 3:2.5341371353453646\n",
      "\n",
      "\n",
      "loss for fold 87: \n",
      " Regressor 1: 5.421101749607104\n",
      " Regressor 2:0.265098577061778\n",
      "Regressor 3:2.7810311685878304\n",
      "\n",
      "\n",
      "loss for fold 88: \n",
      " Regressor 1: 6.237649817951663\n",
      " Regressor 2:0.4298253010705473\n",
      "Regressor 3:3.076224567499752\n",
      "\n",
      "\n",
      "loss for fold 89: \n",
      " Regressor 1: 7.05541270350308\n",
      " Regressor 2:0.609299658034308\n",
      "Regressor 3:3.2666244905436823\n",
      "\n",
      "\n",
      "loss for fold 90: \n",
      " Regressor 1: 7.906318217684429\n",
      " Regressor 2:0.802765450470039\n",
      "Regressor 3:3.422340385038911\n",
      "\n",
      "\n",
      "loss for fold 91: \n",
      " Regressor 1: 8.79026863935502\n",
      " Regressor 2:1.0083205527730792\n",
      "Regressor 3:3.5407690245174406\n",
      "\n",
      "\n",
      "loss for fold 92: \n",
      " Regressor 1: 9.708231254265016\n",
      " Regressor 2:1.2284340059811045\n",
      "Regressor 3:3.6175471405899478\n",
      "\n",
      "\n",
      "loss for fold 93: \n",
      " Regressor 1: 10.6577556927151\n",
      " Regressor 2:1.4627523599471397\n",
      "Regressor 3:3.667894269384046\n",
      "\n",
      "\n",
      "loss for fold 94: \n",
      " Regressor 1: 11.639456533551638\n",
      " Regressor 2:1.70922434722105\n",
      "Regressor 3:3.6784702034523455\n",
      "\n",
      "\n",
      "loss for fold 95: \n",
      " Regressor 1: 12.655050870257071\n",
      " Regressor 2:1.9691864540506727\n",
      "Regressor 3:3.6168670959723555\n",
      "\n",
      "\n",
      "loss for fold 96: \n",
      " Regressor 1: 13.704810012707853\n",
      " Regressor 2:2.2435182499864115\n",
      "Regressor 3:3.501263410109175\n",
      "\n",
      "\n",
      "loss for fold 97: \n",
      " Regressor 1: 4.393053758526875\n",
      " Regressor 2:4.477995041401137\n",
      "Regressor 3:2.899554150260501\n",
      "\n",
      "\n",
      "loss for fold 98: \n",
      " Regressor 1: 1.8472043669791915\n",
      " Regressor 2:1.8897830144461842\n",
      "Regressor 3:1.8743391002577496\n",
      "\n",
      "\n",
      "loss for fold 99: \n",
      " Regressor 1: 4.158187244413185\n",
      " Regressor 2:3.557981677379969\n",
      "Regressor 3:0.4069856153858211\n",
      "\n",
      "\n",
      "loss for fold 100: \n",
      " Regressor 1: 14.520239910045284\n",
      " Regressor 2:11.016750827780152\n",
      "Regressor 3:0.6079677381753896\n",
      "\n",
      "\n",
      "loss for fold 101: \n",
      " Regressor 1: 4.195430595498976\n",
      " Regressor 2:4.258218350177174\n",
      "Regressor 3:2.9456372080431894\n",
      "\n",
      "\n",
      "loss for fold 102: \n",
      " Regressor 1: 3.8636550097569824\n",
      " Regressor 2:3.972592929072718\n",
      "Regressor 3:2.9819607919716145\n",
      "\n",
      "\n",
      "loss for fold 103: \n",
      " Regressor 1: 3.5536740400056814\n",
      " Regressor 2:3.7032116173434027\n",
      "Regressor 3:3.000273287768259\n",
      "\n",
      "\n",
      "loss for fold 104: \n",
      " Regressor 1: 3.26571829443607\n",
      " Regressor 2:3.3337765552486474\n",
      "Regressor 3:2.9995011881427587\n",
      "\n",
      "\n",
      "loss for fold 105: \n",
      " Regressor 1: 2.743374738344997\n",
      " Regressor 2:2.834847494921206\n",
      "Regressor 3:2.982586007524917\n",
      "\n",
      "\n",
      "loss for fold 106: \n",
      " Regressor 1: 2.508564927885871\n",
      " Regressor 2:2.6564151915615177\n",
      "Regressor 3:2.9496502764727186\n",
      "\n",
      "\n",
      "loss for fold 107: \n",
      " Regressor 1: 2.29567402803751\n",
      " Regressor 2:2.491688602497579\n",
      "Regressor 3:2.9019037200432134\n",
      "\n",
      "\n",
      "loss for fold 108: \n",
      " Regressor 1: 2.10516464233239\n",
      " Regressor 2:2.341479636149348\n",
      "Regressor 3:2.839444900086855\n",
      "\n",
      "\n",
      "loss for fold 109: \n",
      " Regressor 1: 1.9365654618510533\n",
      " Regressor 2:2.205146909765702\n",
      "Regressor 3:2.76436122388872\n",
      "\n",
      "\n",
      "loss for fold 110: \n",
      " Regressor 1: 1.7899035479725356\n",
      " Regressor 2:2.083765049830024\n",
      "Regressor 3:2.6771030221107566\n",
      "\n",
      "\n",
      "loss for fold 111: \n",
      " Regressor 1: 1.665483268771439\n",
      " Regressor 2:1.977072194979506\n",
      "Regressor 3:2.5783624453784455\n",
      "\n",
      "\n",
      "loss for fold 112: \n",
      " Regressor 1: 1.5626438612495264\n",
      " Regressor 2:1.884087088232885\n",
      "Regressor 3:2.470213836115308\n",
      "\n",
      "\n",
      "loss for fold 113: \n",
      " Regressor 1: 1.4819882146724055\n",
      " Regressor 2:1.8067565683988818\n",
      "Regressor 3:2.3525685884268874\n",
      "\n",
      "\n",
      "loss for fold 114: \n",
      " Regressor 1: 1.423614572105997\n",
      " Regressor 2:1.735352947119619\n",
      "Regressor 3:2.226999918965131\n",
      "\n",
      "\n",
      "loss for fold 115: \n",
      " Regressor 1: 1.387150893496809\n",
      " Regressor 2:1.6807400781060622\n",
      "Regressor 3:2.093772567265095\n",
      "\n",
      "\n",
      "loss for fold 116: \n",
      " Regressor 1: 1.3731932628031487\n",
      " Regressor 2:1.6438665718116496\n",
      "Regressor 3:1.9551650171658075\n",
      "\n",
      "\n",
      "loss for fold 117: \n",
      " Regressor 1: 1.3805916122887325\n",
      " Regressor 2:1.6239349367571911\n",
      "Regressor 3:1.8111998474261206\n",
      "\n",
      "\n",
      "loss for fold 118: \n",
      " Regressor 1: 1.4105619841318031\n",
      " Regressor 2:1.6210128516030302\n",
      "Regressor 3:1.6625377718068464\n",
      "\n",
      "\n",
      "loss for fold 119: \n",
      " Regressor 1: 1.4624168087859104\n",
      " Regressor 2:1.6357423741489683\n",
      "Regressor 3:1.510928610856574\n",
      "\n",
      "\n",
      "loss for fold 120: \n",
      " Regressor 1: 1.5360650668180966\n",
      " Regressor 2:1.667497041633144\n",
      "Regressor 3:1.3569556420509556\n",
      "\n",
      "\n",
      "loss for fold 121: \n",
      " Regressor 1: 1.6319312476576258\n",
      " Regressor 2:1.7164196988249074\n",
      "Regressor 3:1.2018752707975082\n",
      "\n",
      "\n",
      "loss for fold 122: \n",
      " Regressor 1: 1.749929755512639\n",
      " Regressor 2:1.783233285047075\n",
      "Regressor 3:1.0465698389097682\n",
      "\n",
      "\n",
      "loss for fold 123: \n",
      " Regressor 1: 1.8895084848359218\n",
      " Regressor 2:1.8666240518915558\n",
      "Regressor 3:0.892037009154496\n",
      "\n",
      "\n",
      "loss for fold 124: \n",
      " Regressor 1: 2.0517049433419388\n",
      " Regressor 2:1.9676558762973428\n",
      "Regressor 3:0.7389835318587608\n",
      "\n",
      "\n",
      "loss for fold 125: \n",
      " Regressor 1: 2.2354807600632753\n",
      " Regressor 2:2.08531127264094\n",
      "Regressor 3:0.5891118659442753\n",
      "\n",
      "\n",
      "loss for fold 126: \n",
      " Regressor 1: 2.441738244022069\n",
      " Regressor 2:2.2201169293491105\n",
      "Regressor 3:0.44202010310637974\n",
      "\n",
      "\n",
      "loss for fold 127: \n",
      " Regressor 1: 2.669856603197836\n",
      " Regressor 2:2.3728912673649987\n",
      "Regressor 3:0.30031784267848494\n",
      "\n",
      "\n",
      "loss for fold 128: \n",
      " Regressor 1: 2.919952713060006\n",
      " Regressor 2:2.542707233270196\n",
      "Regressor 3:0.16466696850584484\n",
      "\n",
      "\n",
      "loss for fold 129: \n",
      " Regressor 1: 3.1920982012523282\n",
      " Regressor 2:2.729654461810595\n",
      "Regressor 3:0.03445703936348821\n",
      "\n",
      "\n",
      "loss for fold 130: \n",
      " Regressor 1: 3.4863118385028358\n",
      " Regressor 2:2.934002919385957\n",
      "Regressor 3:0.08736048190097989\n",
      "\n",
      "\n",
      "loss for fold 131: \n",
      " Regressor 1: 3.802352179350919\n",
      " Regressor 2:3.223947839722147\n",
      "Regressor 3:0.20092070827779196\n",
      "\n",
      "\n",
      "loss for fold 132: \n",
      " Regressor 1: 4.142625510288596\n",
      " Regressor 2:3.465524045716279\n",
      "Regressor 3:0.3046558588553374\n",
      "\n",
      "\n",
      "loss for fold 133: \n",
      " Regressor 1: 4.503673849753511\n",
      " Regressor 2:3.7254850398607964\n",
      "Regressor 3:0.3986343282993765\n",
      "\n",
      "\n",
      "loss for fold 134: \n",
      " Regressor 1: 4.885336782294516\n",
      " Regressor 2:4.005443687068187\n",
      "Regressor 3:0.4797493179479737\n",
      "\n",
      "\n",
      "loss for fold 135: \n",
      " Regressor 1: 5.290133572068569\n",
      " Regressor 2:4.3082046598681405\n",
      "Regressor 3:0.5489023554324817\n",
      "\n",
      "\n",
      "loss for fold 136: \n",
      " Regressor 1: 5.7172378023195165\n",
      " Regressor 2:4.625339909555668\n",
      "Regressor 3:0.6045462627056963\n",
      "\n",
      "\n",
      "loss for fold 137: \n",
      " Regressor 1: 6.164750531981291\n",
      " Regressor 2:4.960537758953196\n",
      "Regressor 3:0.6465908621827516\n",
      "\n",
      "\n",
      "loss for fold 138: \n",
      " Regressor 1: 6.637543273350296\n",
      " Regressor 2:5.3154345867687525\n",
      "Regressor 3:0.6725950323978225\n",
      "\n",
      "\n",
      "loss for fold 139: \n",
      " Regressor 1: 7.180950218541859\n",
      " Regressor 2:5.6844268270025236\n",
      "Regressor 3:0.681405571888483\n",
      "\n",
      "\n",
      "loss for fold 140: \n",
      " Regressor 1: 7.705532415005678\n",
      " Regressor 2:6.071864192077157\n",
      "Regressor 3:0.6728228413006363\n",
      "\n",
      "\n",
      "loss for fold 141: \n",
      " Regressor 1: 8.250104012918413\n",
      " Regressor 2:6.475803163482009\n",
      "Regressor 3:0.6465237865283946\n",
      "\n",
      "\n",
      "loss for fold 142: \n",
      " Regressor 1: 8.92882106795156\n",
      " Regressor 2:6.897737159756154\n",
      "Regressor 3:0.6004549256619818\n",
      "\n",
      "\n",
      "loss for fold 143: \n",
      " Regressor 1: 9.583861624240178\n",
      " Regressor 2:7.3360477850220835\n",
      "Regressor 3:0.5340364540959222\n",
      "\n",
      "\n",
      "loss for fold 144: \n",
      " Regressor 1: 10.200248498631261\n",
      " Regressor 2:7.792317985853677\n",
      "Regressor 3:0.4462656986241065\n",
      "\n",
      "\n",
      "loss for fold 145: \n",
      " Regressor 1: 10.83731917428424\n",
      " Regressor 2:8.265274320473731\n",
      "Regressor 3:0.3361412290947676\n",
      "\n",
      "\n",
      "loss for fold 146: \n",
      " Regressor 1: 11.49958447423478\n",
      " Regressor 2:8.755086637908684\n",
      "Regressor 3:0.20340671192738569\n",
      "\n",
      "\n",
      "loss for fold 147: \n",
      " Regressor 1: 5.856522167775978\n",
      " Regressor 2:5.083581506339112\n",
      "Regressor 3:4.851426266274128\n",
      "\n",
      "\n",
      "loss for fold 148: \n",
      " Regressor 1: 1.2960905941623437\n",
      " Regressor 2:4.3205084926261\n",
      "Regressor 3:1.2195240579019568\n",
      "\n",
      "\n",
      "loss for fold 149: \n",
      " Regressor 1: 2.885789867120792\n",
      " Regressor 2:1.9501900661280498\n",
      "Regressor 3:3.1663213893431355\n",
      "\n",
      "\n",
      "loss for fold 150: \n",
      " Regressor 1: 4.808224664711844\n",
      " Regressor 2:3.285517612793397\n",
      "Regressor 3:1.7374477746141785\n",
      "\n",
      "\n",
      "loss for fold 151: \n",
      " Regressor 1: 1.4709022813913606\n",
      " Regressor 2:1.6642910770480484\n",
      "Regressor 3:3.55179764054237\n",
      "\n",
      "\n",
      "loss for fold 152: \n",
      " Regressor 1: 4.7289357598924795\n",
      " Regressor 2:4.948752192513787\n",
      "Regressor 3:4.012597392711584\n",
      "\n",
      "\n",
      "loss for fold 153: \n",
      " Regressor 1: 4.113248944668953\n",
      " Regressor 2:4.828348685050269\n",
      "Regressor 3:3.2103753745158805\n",
      "\n",
      "\n",
      "loss for fold 154: \n",
      " Regressor 1: 3.5176475528810656\n",
      " Regressor 2:4.710742434658005\n",
      "Regressor 3:2.462532173025913\n",
      "\n",
      "\n",
      "loss for fold 155: \n",
      " Regressor 1: 2.8705214203763134\n",
      " Regressor 2:4.595466697861752\n",
      "Regressor 3:1.771830308208898\n",
      "\n",
      "\n",
      "loss for fold 156: \n",
      " Regressor 1: 2.3178183566265744\n",
      " Regressor 2:4.482751709834435\n",
      "Regressor 3:1.1344650038675042\n",
      "\n",
      "\n",
      "loss for fold 157: \n",
      " Regressor 1: 1.7858544347673941\n",
      " Regressor 2:4.37219823494268\n",
      "Regressor 3:0.5491191498998766\n",
      "\n",
      "\n",
      "loss for fold 158: \n",
      " Regressor 1: 1.2748439379340013\n",
      " Regressor 2:4.263864976356167\n",
      "Regressor 3:0.015251807054315947\n",
      "\n",
      "\n",
      "loss for fold 159: \n",
      " Regressor 1: 0.7839606165339106\n",
      " Regressor 2:4.157905849120603\n",
      "Regressor 3:0.47071598243898904\n",
      "\n",
      "\n",
      "loss for fold 160: \n",
      " Regressor 1: 0.3136537597629854\n",
      " Regressor 2:4.0542796492504465\n",
      "Regressor 3:0.9085434388489695\n",
      "\n",
      "\n",
      "loss for fold 161: \n",
      " Regressor 1: 0.1366900441624992\n",
      " Regressor 2:3.951815716958219\n",
      "Regressor 3:1.299789457901941\n",
      "\n",
      "\n",
      "loss for fold 162: \n",
      " Regressor 1: 0.5662724507627743\n",
      " Regressor 2:3.8542287883813167\n",
      "Regressor 3:1.6471241902172977\n",
      "\n",
      "\n",
      "loss for fold 163: \n",
      " Regressor 1: 0.975496141693462\n",
      " Regressor 2:3.757293762639563\n",
      "Regressor 3:1.9506577521348696\n",
      "\n",
      "\n",
      "loss for fold 164: \n",
      " Regressor 1: 1.363829690256182\n",
      " Regressor 2:3.663948861331445\n",
      "Regressor 3:2.213310303198231\n",
      "\n",
      "\n",
      "loss for fold 165: \n",
      " Regressor 1: 1.7316234576825522\n",
      " Regressor 2:3.5726694561174988\n",
      "Regressor 3:2.4357642404261384\n",
      "\n",
      "\n",
      "loss for fold 166: \n",
      " Regressor 1: 2.0799679260043575\n",
      " Regressor 2:3.4840106955085446\n",
      "Regressor 3:2.619168487342069\n",
      "\n",
      "\n",
      "loss for fold 167: \n",
      " Regressor 1: 2.4069467342805027\n",
      " Regressor 2:3.396742340606931\n",
      "Regressor 3:2.7668477936026754\n",
      "\n",
      "\n",
      "loss for fold 168: \n",
      " Regressor 1: 2.7139925011997974\n",
      " Regressor 2:3.3131482592994637\n",
      "Regressor 3:2.8782171834359715\n",
      "\n",
      "\n",
      "loss for fold 169: \n",
      " Regressor 1: 3.115155593733803\n",
      " Regressor 2:3.2309521156555796\n",
      "Regressor 3:2.955493895400643\n",
      "\n",
      "\n",
      "loss for fold 170: \n",
      " Regressor 1: 3.380207867596681\n",
      " Regressor 2:3.151833171515591\n",
      "Regressor 3:3.043631937210879\n",
      "\n",
      "\n",
      "loss for fold 171: \n",
      " Regressor 1: 3.625724607459567\n",
      " Regressor 2:3.074459437255147\n",
      "Regressor 3:3.05787209695967\n",
      "\n",
      "\n",
      "loss for fold 172: \n",
      " Regressor 1: 3.850493819494986\n",
      " Regressor 2:2.9250949922830003\n",
      "Regressor 3:3.0425692933210584\n",
      "\n",
      "\n",
      "loss for fold 173: \n",
      " Regressor 1: 4.054522112573551\n",
      " Regressor 2:2.852987895840034\n",
      "Regressor 3:2.957002505838908\n",
      "\n",
      "\n",
      "loss for fold 174: \n",
      " Regressor 1: 4.2388339975162594\n",
      " Regressor 2:2.7836201582241475\n",
      "Regressor 3:2.8874425736929403\n",
      "\n",
      "\n",
      "loss for fold 175: \n",
      " Regressor 1: 4.402196288765552\n",
      " Regressor 2:2.7160554920394357\n",
      "Regressor 3:2.7927246367119167\n",
      "\n",
      "\n",
      "loss for fold 176: \n",
      " Regressor 1: 4.545139995186858\n",
      " Regressor 2:2.650901193097109\n",
      "Regressor 3:2.675489237102049\n",
      "\n",
      "\n",
      "loss for fold 177: \n",
      " Regressor 1: 4.667596072627333\n",
      " Regressor 2:2.588509485150867\n",
      "Regressor 3:2.5364581209042854\n",
      "\n",
      "\n",
      "loss for fold 178: \n",
      " Regressor 1: 4.7698926941693\n",
      " Regressor 2:2.5282061517196084\n",
      "Regressor 3:2.3768554794196497\n",
      "\n",
      "\n",
      "loss for fold 179: \n",
      " Regressor 1: 4.851465008267766\n",
      " Regressor 2:2.4702329684887285\n",
      "Regressor 3:2.1982049749876573\n",
      "\n",
      "\n",
      "loss for fold 180: \n",
      " Regressor 1: 4.91326749523347\n",
      " Regressor 2:2.4150174437111094\n",
      "Regressor 3:2.0019995583348056\n",
      "\n",
      "\n",
      "loss for fold 181: \n",
      " Regressor 1: 4.953402644299388\n",
      " Regressor 2:2.3607591060062294\n",
      "Regressor 3:1.790660214614178\n",
      "\n",
      "\n",
      "loss for fold 182: \n",
      " Regressor 1: 4.9736188143469064\n",
      " Regressor 2:2.311083194207585\n",
      "Regressor 3:1.5644158112261124\n",
      "\n",
      "\n",
      "loss for fold 183: \n",
      " Regressor 1: 4.973820511039094\n",
      " Regressor 2:2.2627211709302255\n",
      "Regressor 3:1.325936087954517\n",
      "\n",
      "\n",
      "loss for fold 184: \n",
      " Regressor 1: 4.95295048263371\n",
      " Regressor 2:2.21675782347673\n",
      "Regressor 3:1.076453128418887\n",
      "\n",
      "\n",
      "loss for fold 185: \n",
      " Regressor 1: 4.911530990049528\n",
      " Regressor 2:2.1729301392382396\n",
      "Regressor 3:0.8173781428865148\n",
      "\n",
      "\n",
      "loss for fold 186: \n",
      " Regressor 1: 4.865242744331837\n",
      " Regressor 2:2.1316117460027604\n",
      "Regressor 3:0.5497876615352979\n",
      "\n",
      "\n",
      "loss for fold 187: \n",
      " Regressor 1: 4.783655967046002\n",
      " Regressor 2:2.092954049565602\n",
      "Regressor 3:0.27637557958007974\n",
      "\n",
      "\n",
      "loss for fold 188: \n",
      " Regressor 1: 4.68225074805126\n",
      " Regressor 2:2.0566285016753323\n",
      "Regressor 3:0.0032562595861058696\n",
      "\n",
      "\n",
      "loss for fold 189: \n",
      " Regressor 1: 4.559949892458221\n",
      " Regressor 2:2.022185422179179\n",
      "Regressor 3:0.2864093488679522\n",
      "\n",
      "\n",
      "loss for fold 190: \n",
      " Regressor 1: 4.418241178360162\n",
      " Regressor 2:1.991021917704785\n",
      "Regressor 3:0.5712888079326568\n",
      "\n",
      "\n",
      "loss for fold 191: \n",
      " Regressor 1: 4.255077623622654\n",
      " Regressor 2:1.9619700142860905\n",
      "Regressor 3:0.8562252957716474\n",
      "\n",
      "\n",
      "loss for fold 192: \n",
      " Regressor 1: 4.071886029036776\n",
      " Regressor 2:1.935430304888202\n",
      "Regressor 3:1.1409814261660927\n",
      "\n",
      "\n",
      "loss for fold 193: \n",
      " Regressor 1: 3.8896201426179715\n",
      " Regressor 2:1.9112638540269131\n",
      "Regressor 3:1.4232683626995897\n",
      "\n",
      "\n",
      "loss for fold 194: \n",
      " Regressor 1: 3.666702701789916\n",
      " Regressor 2:1.888962731459749\n",
      "Regressor 3:1.7013314374617519\n",
      "\n",
      "\n",
      "loss for fold 195: \n",
      " Regressor 1: 3.422092043286625\n",
      " Regressor 2:1.869496251020227\n",
      "Regressor 3:1.9737518765866024\n",
      "\n",
      "\n",
      "loss for fold 196: \n",
      " Regressor 1: 3.1584840489341843\n",
      " Regressor 2:1.8513779045631504\n",
      "Regressor 3:2.2400698039696447\n",
      "\n",
      "\n",
      "loss for fold 197: \n",
      " Regressor 1: 2.245822621882951\n",
      " Regressor 2:1.819672238838064\n",
      "Regressor 3:2.862498431833208\n",
      "\n",
      "\n",
      "loss for fold 198: \n",
      " Regressor 1: 13.513564760859996\n",
      " Regressor 2:6.304357994103789\n",
      "Regressor 3:2.5275172819974863\n",
      "\n",
      "\n",
      "loss for fold 199: \n",
      " Regressor 1: 33.624492208611116\n",
      " Regressor 2:7.446884412366444\n",
      "Regressor 3:5.017394325119106\n",
      "\n",
      "\n",
      "loss for fold 200: \n",
      " Regressor 1: 16.494803354635543\n",
      " Regressor 2:2.3242203285647633\n",
      "Regressor 3:3.749980934752248\n",
      "\n",
      "\n",
      "loss for fold 201: \n",
      " Regressor 1: 16.868406109274922\n",
      " Regressor 2:3.352013965418923\n",
      "Regressor 3:3.045876140342134\n",
      "\n",
      "\n",
      "loss for fold 202: \n",
      " Regressor 1: 23.527552794629337\n",
      " Regressor 2:1.269799784343551\n",
      "Regressor 3:0.8648254598317848\n",
      "\n",
      "\n",
      "loss for fold 203: \n",
      " Regressor 1: 29.746073689979042\n",
      " Regressor 2:0.6491113728234232\n",
      "Regressor 3:3.3213629360075494\n",
      "\n",
      "\n",
      "loss for fold 204: \n",
      " Regressor 1: 12.699755565332698\n",
      " Regressor 2:4.681757059432783\n",
      "Regressor 3:3.7689525108611015\n",
      "\n",
      "\n",
      "loss for fold 205: \n",
      " Regressor 1: 12.149921437540645\n",
      " Regressor 2:4.435448875244823\n",
      "Regressor 3:3.346469178127961\n",
      "\n",
      "\n",
      "loss for fold 206: \n",
      " Regressor 1: 11.58520991733717\n",
      " Regressor 2:4.190699835704564\n",
      "Regressor 3:2.9154436834799\n",
      "\n",
      "\n",
      "loss for fold 207: \n",
      " Regressor 1: 11.005615523139731\n",
      " Regressor 2:3.948173737044293\n",
      "Regressor 3:2.5192483009880426\n",
      "\n",
      "\n",
      "loss for fold 208: \n",
      " Regressor 1: 10.408873568253313\n",
      " Regressor 2:3.7080448616572284\n",
      "Regressor 3:2.13775201404189\n",
      "\n",
      "\n",
      "loss for fold 209: \n",
      " Regressor 1: 9.798010621936822\n",
      " Regressor 2:3.4702722437848124\n",
      "Regressor 3:1.7690740922842672\n",
      "\n",
      "\n",
      "loss for fold 210: \n",
      " Regressor 1: 9.169529616173161\n",
      " Regressor 2:3.2336940828129137\n",
      "Regressor 3:1.4150668918736464\n",
      "\n",
      "\n",
      "loss for fold 211: \n",
      " Regressor 1: 8.526477780663594\n",
      " Regressor 2:2.769649991522435\n",
      "Regressor 3:1.0736047044575407\n",
      "\n",
      "\n",
      "loss for fold 212: \n",
      " Regressor 1: 7.868704630754895\n",
      " Regressor 2:2.5421814541307413\n",
      "Regressor 3:0.7456768359248329\n",
      "\n",
      "\n",
      "loss for fold 213: \n",
      " Regressor 1: 7.193249207816482\n",
      " Regressor 2:2.3160083699274345\n",
      "Regressor 3:0.43232987908479004\n",
      "\n",
      "\n",
      "loss for fold 214: \n",
      " Regressor 1: 6.503510297277842\n",
      " Regressor 2:2.0926169554645107\n",
      "Regressor 3:0.13167206166146528\n",
      "\n",
      "\n",
      "loss for fold 215: \n",
      " Regressor 1: 5.797873432756511\n",
      " Regressor 2:1.870461716274903\n",
      "Regressor 3:0.15541656825978478\n",
      "\n",
      "\n",
      "loss for fold 216: \n",
      " Regressor 1: 5.076099926946441\n",
      " Regressor 2:1.650965125122596\n",
      "Regressor 3:0.42867841496981995\n",
      "\n",
      "\n",
      "loss for fold 217: \n",
      " Regressor 1: 4.3392440876991785\n",
      " Regressor 2:1.4332115923359368\n",
      "Regressor 3:0.6876752399979047\n",
      "\n",
      "\n",
      "loss for fold 218: \n",
      " Regressor 1: 3.5860944067366773\n",
      " Regressor 2:1.2179706554381404\n",
      "Regressor 3:0.9342541615225741\n",
      "\n",
      "\n",
      "loss for fold 219: \n",
      " Regressor 1: 2.2310223445344626\n",
      " Regressor 2:1.004614831967217\n",
      "Regressor 3:1.1666816299665683\n",
      "\n",
      "\n",
      "loss for fold 220: \n",
      " Regressor 1: 1.4471387062105165\n",
      " Regressor 2:0.7926999841537636\n",
      "Regressor 3:1.3855691695153851\n",
      "\n",
      "\n",
      "loss for fold 221: \n",
      " Regressor 1: 0.6479508085986829\n",
      " Regressor 2:0.5834004640805652\n",
      "Regressor 3:1.5909275363582616\n",
      "\n",
      "\n",
      "loss for fold 222: \n",
      " Regressor 1: 0.1672427141989168\n",
      " Regressor 2:0.37659026853679833\n",
      "Regressor 3:1.782231865055234\n",
      "\n",
      "\n",
      "loss for fold 223: \n",
      " Regressor 1: 0.9984854062041606\n",
      " Regressor 2:0.17144258545211244\n",
      "Regressor 3:1.9603962367347023\n",
      "\n",
      "\n",
      "loss for fold 224: \n",
      " Regressor 1: 1.8451129362839502\n",
      " Regressor 2:0.031954676789499814\n",
      "Regressor 3:2.1247022668489883\n",
      "\n",
      "\n",
      "loss for fold 225: \n",
      " Regressor 1: 2.7074682028064245\n",
      " Regressor 2:0.2330806059009518\n",
      "Regressor 3:2.275407132294742\n",
      "\n",
      "\n",
      "loss for fold 226: \n",
      " Regressor 1: 3.8777027751963367\n",
      " Regressor 2:0.43170909727217577\n",
      "Regressor 3:2.4129570587356106\n",
      "\n",
      "\n",
      "loss for fold 227: \n",
      " Regressor 1: 4.772238423985421\n",
      " Regressor 2:0.6287636912553225\n",
      "Regressor 3:2.536558516087979\n",
      "\n",
      "\n",
      "loss for fold 228: \n",
      " Regressor 1: 5.6813015534188125\n",
      " Regressor 2:0.8237537709297627\n",
      "Regressor 3:2.647171713769467\n",
      "\n",
      "\n",
      "loss for fold 229: \n",
      " Regressor 1: 6.606725546866507\n",
      " Regressor 2:1.0165127335693995\n",
      "Regressor 3:2.7434470718758064\n",
      "\n",
      "\n",
      "loss for fold 230: \n",
      " Regressor 1: 7.547275190140333\n",
      " Regressor 2:1.206951303882775\n",
      "Regressor 3:2.8261240932859053\n",
      "\n",
      "\n",
      "loss for fold 231: \n",
      " Regressor 1: 8.504028189942503\n",
      " Regressor 2:1.3955586462493805\n",
      "Regressor 3:2.8956142691199993\n",
      "\n",
      "\n",
      "loss for fold 232: \n",
      " Regressor 1: 9.47559660478515\n",
      " Regressor 2:1.5820850357411338\n",
      "Regressor 3:2.9511382825339787\n",
      "\n",
      "\n",
      "loss for fold 233: \n",
      " Regressor 1: 10.46314731553856\n",
      " Regressor 2:1.7671458431815878\n",
      "Regressor 3:2.9930317184538495\n",
      "\n",
      "\n",
      "loss for fold 234: \n",
      " Regressor 1: 11.4668410885334\n",
      " Regressor 2:1.9489303652527958\n",
      "Regressor 3:3.028568338483419\n",
      "\n",
      "\n",
      "loss for fold 235: \n",
      " Regressor 1: 12.48617933860777\n",
      " Regressor 2:2.129260925245287\n",
      "Regressor 3:3.043604796162457\n",
      "\n",
      "\n",
      "loss for fold 236: \n",
      " Regressor 1: 13.5206493125301\n",
      " Regressor 2:2.308137974044513\n",
      "Regressor 3:3.044787374600876\n",
      "\n",
      "\n",
      "loss for fold 237: \n",
      " Regressor 1: 14.571021745561875\n",
      " Regressor 2:2.4840452979063024\n",
      "Regressor 3:3.032497224387498\n",
      "\n",
      "\n",
      "loss for fold 238: \n",
      " Regressor 1: 15.637135454416864\n",
      " Regressor 2:2.6581974083382853\n",
      "Regressor 3:3.006397398379132\n",
      "\n",
      "\n",
      "loss for fold 239: \n",
      " Regressor 1: 16.719334300682615\n",
      " Regressor 2:2.830012988051184\n",
      "Regressor 3:2.9605769128984107\n",
      "\n",
      "\n",
      "loss for fold 240: \n",
      " Regressor 1: 17.816221298160215\n",
      " Regressor 2:3.118373883378606\n",
      "Regressor 3:2.9077250481096257\n",
      "\n",
      "\n",
      "loss for fold 241: \n",
      " Regressor 1: 18.92984180767937\n",
      " Regressor 2:3.2902640875359346\n",
      "Regressor 3:2.841158300647109\n",
      "\n",
      "\n",
      "loss for fold 242: \n",
      " Regressor 1: 20.05847259958289\n",
      " Regressor 2:3.460016214233862\n",
      "Regressor 3:2.7604616945006946\n",
      "\n",
      "\n",
      "loss for fold 243: \n",
      " Regressor 1: 21.20273728661587\n",
      " Regressor 2:3.6274993192367226\n",
      "Regressor 3:2.6670800995176904\n",
      "\n",
      "\n",
      "loss for fold 244: \n",
      " Regressor 1: 22.36292035231498\n",
      " Regressor 2:3.7936674583596233\n",
      "Regressor 3:2.559463525336227\n",
      "\n",
      "\n",
      "loss for fold 245: \n",
      " Regressor 1: 23.538943634132515\n",
      " Regressor 2:3.956201883325278\n",
      "Regressor 3:2.4384572693220825\n",
      "\n",
      "\n",
      "loss for fold 246: \n",
      " Regressor 1: 24.730749841497026\n",
      " Regressor 2:4.117712330685379\n",
      "Regressor 3:2.304229120876643\n",
      "\n",
      "\n",
      "loss for fold 247: \n",
      " Regressor 1: 25.937014287287703\n",
      " Regressor 2:4.278478314557891\n",
      "Regressor 3:2.156014481680959\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#The following script divides data into attributes and labels:\n",
    "X = dataset.iloc[:, :16].values\n",
    "y = dataset.iloc[:, 16:].values\n",
    "\n",
    "\n",
    "loss_per_fold_0 = []\n",
    "loss_per_fold_1 = []\n",
    "loss_per_fold_2 = []\n",
    "\n",
    "\n",
    "# Create the SVR regressor\n",
    "svr = SVR(kernel='linear',epsilon=3)\n",
    "# Create the Multioutput Regressor\n",
    "mor = MultiOutputRegressor(svr)\n",
    "\n",
    "    \n",
    "#Divide the data into training and testing sets for KFold Cross validaton loop\n",
    "#Define the K-fold Cross Validator\n",
    "cv = KFold(n_splits=247, shuffle=False)\n",
    "cv.get_n_splits(X)\n",
    "print(cv,\"\\n\")\n",
    "\n",
    "# K-fold Cross Validation model evaluation\n",
    "fold_no = 1\n",
    "\n",
    "\n",
    "for train_index, test_index in cv.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    # Feature Scaling\n",
    "    sc = StandardScaler()\n",
    "    X_train = sc.fit_transform(X_train)\n",
    "    X_test = sc.transform(X_test)\n",
    "    \n",
    "    # Train the regressor\n",
    "    mor = mor.fit(X_train, y_train)\n",
    "\n",
    "    # Generate predictions for testing data\n",
    "    y_pred = mor.predict(X_test)\n",
    "\n",
    "\n",
    "     # Generate generalization metrics\n",
    "    \n",
    "    print(f'loss for fold {fold_no}: \\n Regressor 1: {metrics.mean_absolute_error(y_test[:,0], y_pred[:,0])}\\n Regressor 2:{metrics.mean_absolute_error(y_test[:,1], y_pred[:,1])}\\nRegressor 3:{metrics.mean_absolute_error(y_test[:,2], y_pred[:,2])}\\n\\n')\n",
    "\n",
    "    loss_per_fold_0.append(metrics.mean_absolute_error(y_test[:,0], y_pred[:,0]))\n",
    "    loss_per_fold_1.append(metrics.mean_absolute_error(y_test[:,1], y_pred[:,1]))\n",
    "    loss_per_fold_2.append(metrics.mean_absolute_error(y_test[:,2], y_pred[:,2]))\n",
    "\n",
    "    # Increase fold number\n",
    "    fold_no = fold_no + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Average scores for all folds:\n> Mean loss fisrt regressor: 5.2948514163919755\n> Mean loss fisrt regressor: 2.4271987561501827\n> Mean loss fisrt regressor: 2.4422704430032094\n------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print('Average scores for all folds:')\n",
    "print(f'> Mean loss fisrt regressor: {np.mean(loss_per_fold_0)}')\n",
    "print(f'> Mean loss fisrt regressor: {np.mean(loss_per_fold_1)}')\n",
    "print(f'> Mean loss fisrt regressor: {np.mean(loss_per_fold_2)}')\n",
    "print('------------------------------------------------------------------------')\n"
   ]
  }
 ]
}